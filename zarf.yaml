# yaml-language-server: $schema=https://raw.githubusercontent.com/defenseunicorns/zarf/main/zarf.schema.json

kind: ZarfPackageConfig
metadata:
  name: uds-k3d
  description: "UDS K3d Cluster Setup. WARNING: This will destroy the cluster if it already exists."
  url: https://github.com/justinthelaw/uds-k3d
  yolo: true
  # x-release-please-start-version
  version: "0.9.0"
  # x-release-please-end

variables:
  - name: CLUSTER_NAME
    description: "Name of the cluster"
    default: "uds"

  - name: K3S_IMAGE_REPOSITORY
    description: "K3s image repository to use"
    default: "rancher/k3s"

  - name: K3S_IMAGE_VERSION
    description: "K3d image version to use"
    default: "v1.30.4-k3s1"

  - name: CUDA_IMAGE_VERSION
    description: "CUDA image to use"
    default: "12.1.0-base-ubuntu22.04"

  - name: NUMBER_OF_GPUS
    description: "Number of GPUs to passthrough to the K3D cluster"
    default: "all"

  - name: K3D_EXTRA_ARGS
    description: "Optionally pass k3d arguments to the default"
    default: ""

  - name: NGINX_EXTRA_PORTS
    description: "Optionally allow more ports through Nginx (combine with K3D_EXTRA_ARGS '-p <port>:<port>@server:*')"
    default: "[]"

components:
  - name: destroy-cluster
    required: true
    description: "Optionally destroy the cluster before creating it"
    actions:
      onDeploy:
        before:
          - cmd: k3d cluster delete ${ZARF_VAR_CLUSTER_NAME}
            description: "Destroy the cluster"

  - name: set-k3d-image
    required: true
    description: "Set the K3s base image"
    actions:
      onDeploy:
        before:
          - cmd: |
              echo "${ZARF_VAR_K3S_IMAGE_REPOSITORY}:${ZARF_VAR_K3S_IMAGE_VERSION}"
            setVariables:
              - name: K3D_IMAGE

  - name: inject-cuda-image
    required: true
    only:
      flavor: cuda
    description: "Overwrites the K3s base image variable to be the CUDA K3s image"
    actions:
      onDeploy:
        before:
          - cmd: |
              echo "ghcr.io/justinthelaw/uds-k3d/cuda-k3s:${ZARF_VAR_K3S_IMAGE_VERSION}-cuda-${ZARF_VAR_CUDA_IMAGE_VERSION}"
            setVariables:
              - name: K3D_IMAGE

  - name: expose-gpus
    required: true
    only:
      flavor: cuda
    description: "Adds the extra K3d argument for exposing host GPUs to the cluster"
    actions:
      onDeploy:
        before:
          - cmd: |
              echo "${ZARF_VAR_K3D_EXTRA_ARGS} --gpus=${ZARF_VAR_NUMBER_OF_GPUS}"
            setVariables:
              - name: K3D_EXTRA_ARGS

  - name: create-cluster
    required: true
    description: "Create the k3d cluster"
    actions:
      onDeploy:
        before:
          - cmd: |
              k3d cluster create \
              -p "80:80@server:*" \
              -p "443:443@server:*" \
              --api-port 6550 \
              --k3s-arg "--disable=traefik@server:*" \
              --k3s-arg "--disable=metrics-server@server:*" \
              --k3s-arg "--disable=servicelb@server:*" \
              --k3s-arg "--disable=local-storage@server:*" \
              ${ZARF_VAR_K3D_EXTRA_ARGS} \
              --image ${ZARF_VAR_K3D_IMAGE} \
              "${ZARF_VAR_CLUSTER_NAME}"
            description: "Create the cluster"
        onSuccess:
          - cmd: |
              echo "You can access this cluster over SSH (note http redirect will redirect to port 80 instead of 8080):"
              echo "ssh -N -L 8080:localhost:80 -L 8443:localhost:443 -L 6550:localhost:6550"
              echo
              echo "To get the kubeconfig:"
              echo "k3d kubeconfig get ${ZARF_VAR_CLUSTER_NAME}"
              echo
              echo "This cluster can be destroyed with:"
              echo "k3d cluster delete ${ZARF_VAR_CLUSTER_NAME}"
            description: "Print out information about how to access the cluster remotely"

  - name: uds-dev-stack
    required: true
    description: "Install MetalLB, NGINX, Minio, local-path-rwx and Ensure MachineID to meet UDS developer needs without later config changes"
    actions:
      onDeploy:
        before:
          - cmd: uds zarf tools kubectl get nodes -o=jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' | cut -d'.' -f1-3
            description: "Load network base IP for MetalLB"
            setVariables:
              - name: BASE_IP
        after:
          - cmd: uds zarf tools kubectl rollout restart deployment coredns -n kube-system
            description: "Restart CoreDNS to pick up internal DNS override for uds.dev"
    charts:
      - name: metallb
        namespace: uds-dev-stack
        url: https://metallb.github.io/metallb
        version: 0.14.8
      - name: uds-dev-stack
        namespace: uds-dev-stack
        localPath: chart
        # x-release-please-start-version
        version: 0.11.2
        # x-release-please-end
        valuesFiles:
          - "values/dev-stack-values.yaml"
        variables:
          - name: COREDNS_OVERRIDES
            # Defaults contain rewrites of `*.uds.dev` to the UDS core Istio tenant and admin gateways
            description: "CoreDNS overrides"
            path: coreDnsOverrides
      - name: minio
        namespace: uds-dev-stack
        version: 5.2.0
        url: https://charts.min.io/
        valuesFiles:
          - "values/minio-values.yaml"

  - name: nvidia-gpu-operator
    description: "Install the NVIDIA GPU Operator for CUDA-enabled clusters"
    only:
      flavor: cuda
    required: true
    charts:
      - name: gpu-operator
        url: https://helm.ngc.nvidia.com/nvidia
        version: v24.3.0
        namespace: kube-system
        valuesFiles:
          - "values/nvidia-gpu-operator-values.yaml"
    actions:
      onDeploy:
        after:
          - description: "Validate nvidia-device-plugin-daemonset is up"
            wait:
              cluster:
                kind: Pod
                name: app=nvidia-device-plugin-daemonset
                namespace: kube-system
                # Ensure the device plugin is healthy, which might take a while depending on the machine 
                condition: "'{.status.conditions[2].status}'=True"
            maxTotalSeconds: 600
          - description: "Validate nvidia-operator-validator is completed"
            wait:
              cluster:
                kind: Pod
                name: app=nvidia-operator-validator
                namespace: kube-system
                # Ensure the NVIDIA host validator job succeeds
                condition: "'{.status.conditions[2].status}'=True"
            maxTotalSeconds: 300
